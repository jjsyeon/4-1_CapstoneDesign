{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scoring_Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "60753ad5",
        "RpwNFHdtJBvU",
        "z8VmErmuJGwa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2UosdTo5sbx",
        "outputId": "4946c860-53e4-4cc4-8d8a-ed6b5bb91ce6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj69zoyj5q_k",
        "outputId": "cce007d8-2bce-45c1-ece3-dc537cc25d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/SWC_Capstone/5월\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/Shareddrives/SWC_Capstone/5월'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "c9e4d025"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk import tokenize\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "9X4Shv5G6CCT"
      },
      "outputs": [],
      "source": [
        "!pip install -q wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60753ad5"
      },
      "source": [
        "# 텍스트 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "depression = pd.read_csv(\"./psy_data/year_month_col/depress_ym.csv\")\n",
        "bipolar = pd.read_csv(\"./psy_data/year_month_col/bipolar_ym.csv\")\n",
        "panic = pd.read_csv(\"./psy_data/year_month_col/panic_ym.csv\")\n",
        "covid19 = pd.read_csv(\"./psy_data/year_month_col/covid_ym.csv\")\n",
        "relationship = pd.read_csv(\"./psy_data/year_month_col/relationship_ym.csv\")\n",
        "teaching = pd.read_csv(\"./psy_data/year_month_col/teaching_ym.csv\")"
      ],
      "metadata": {
        "id": "jwOtd6jbKn5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f2ba588",
        "outputId": "853b28d2-86b3-401a-e3cc-1b0a75d35da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = list(set(stopwords.words('english')))\n",
        "stop_words.extend([\"'m\"])\n",
        "print()\n",
        "new_stopwords = []\n",
        "temp = ['hadn','mightn','mustn','wasn','couldn','doesn','hasn','ain','shan','aren','weren','wouldn','shouldn']\n",
        "for word in stop_words:\n",
        "    if 'not' in word: continue\n",
        "    elif \"'t\" in word: continue\n",
        "    elif len(word) == 1: continue\n",
        "    elif word in temp: continue\n",
        "    elif '!' in word: continue\n",
        "    elif '.' in word: continue\n",
        "    else: new_stopwords.append(word)\n",
        "        \n",
        "# 감정분석용 cleansing\n",
        "def s_cleansing(List):\n",
        "    count=0\n",
        "    corpus = []\n",
        "    for i in List:\n",
        "        i = str(i)# 정규화 에러로 인해 추가\n",
        "        url_pattern ='https?://\\S+|#([0-9a-zA-Z]*)'\n",
        "        i = re.sub(pattern=url_pattern, repl=' ', string = i)\n",
        "        i = i.replace('\\r','').replace('\\n','')\n",
        "        text = re.sub('[-=+,#/\\?:^$”@*\\\"“※~&%ㆍ!』\\\\‘|\\(\\)\\[\\]\\<\\>`…》;’]', '', i.lower())\n",
        "        corpus.append(text)\n",
        "        count+=1\n",
        "        #if(count>24900): print(count)        \n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8756e07"
      },
      "outputs": [],
      "source": [
        "dep_mix = []\n",
        "for i in (depression['title'] + ' '+depression['text_context']):\n",
        "    dep_mix.append(i)\n",
        "    \n",
        "bipolar_mix = []\n",
        "for i in (bipolar['title'] + ' '+bipolar['text_context']):\n",
        "    bipolar_mix.append(i)\n",
        "    \n",
        "panic_mix = []\n",
        "for i in (panic['title'] + ' '+panic['text_context']):\n",
        "    panic_mix.append(i)\n",
        "\n",
        "covid19_mix = []\n",
        "for i in (covid19['title'] + ' '+covid19['text_context']):\n",
        "    covid19_mix.append(i)\n",
        "    \n",
        "relationship_mix = []\n",
        "for i in (relationship['title'] + ' '+relationship['text_context']):\n",
        "    relationship_mix.append(i)\n",
        "    \n",
        "teaching_mix = []\n",
        "for i in (teaching['title'] + ' '+teaching['text_context']):\n",
        "    teaching_mix.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e44f4a99"
      },
      "outputs": [],
      "source": [
        "dep_cleansing = s_cleansing(dep_mix)\n",
        "bipolar_cleansing = s_cleansing(bipolar_mix)\n",
        "anxiety_cleansing = s_cleansing(panic_mix)\n",
        "\n",
        "covid19_cleansing = s_cleansing(covid19_mix)\n",
        "teaching_cleansing = s_cleansing(teaching_mix)\n",
        "relationship_cleansing = s_cleansing(relationship_mix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Valence-Arousal Scoring Algorithn"
      ],
      "metadata": {
        "id": "F_BxOagw5Ml6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기존 알고리즘"
      ],
      "metadata": {
        "id": "YgWtAHl35U0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 사전 다운로드"
      ],
      "metadata": {
        "id": "kvfdIbFt5Q-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon_Val = pd.read_csv(\"./NRC-VAD-Lexicon-Aug2018Release/OneFilePerDimension/v-scores.txt\", sep = '\\t')\n",
        "lexicon_Val.columns =['Emotion','Score']\n",
        "lexicon_Val=lexicon_Val.dropna()\n",
        "lexicon_Val = lexicon_Val.reset_index()\n",
        "lexicon_Val = lexicon_Val.drop(columns = ['index'])\n",
        "\n",
        "lexicon_Aro = pd.read_csv(\"./NRC-VAD-Lexicon-Aug2018Release/OneFilePerDimension/a-scores.txt\", sep = '\\t')\n",
        "lexicon_Aro.columns =['Emotion','Score']\n",
        "lexicon_Aro=lexicon_Aro.dropna()\n",
        "lexicon_Aro = lexicon_Aro.reset_index()\n",
        "lexicon_Aro = lexicon_Aro.drop(columns = ['index'])\n"
      ],
      "metadata": {
        "id": "grr9fQH15T4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 알고리즘 구현"
      ],
      "metadata": {
        "id": "tYyN7k2IKoex"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4465bf6a"
      },
      "outputs": [],
      "source": [
        "# V,A 중 하나의 데이터셋만 다루는 함수 \n",
        "\n",
        "# selected : tokenize by word original\n",
        "# https://github.com/Priya22/EmotionDynamics\n",
        "\n",
        "# 이후 VADER의 부정어 처리에 맞는 tokenizer 선택\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "def score_by_lex_original(lexicon, text_df, save_path):#lexicon scoring algorithm\n",
        "    numTokens = []\n",
        "    numLexTokens = []\n",
        "    avgLexVal = []\n",
        "    checkpoint = 0\n",
        "    for t in text_df:\n",
        "        score = []\n",
        "        text = str(t)\n",
        "        tokenized = text_to_word_sequence(text)\n",
        "        numTokens.append(len(tokenized))\n",
        "\n",
        "        for token in tokenized:\n",
        "            index = 0\n",
        "            for emotion in lexicon['Emotion']:\n",
        "                if emotion in token:\n",
        "                    score.append(lexicon['Score'][index])\n",
        "                    break                \n",
        "                index += 1\n",
        "                \n",
        "        checkpoint += 1\n",
        "        if checkpoint % 1000 == 0: # 중간 저장 과정\n",
        "            print(check)  \n",
        "            result_DF = pd.DataFrame((zip(numTokens, numLexTokens, avgLexVal)), columns = ['numTokens', 'numLexTokens', 'avgLexVal'])\n",
        "            result_DF.to_csv(save_path, mode='w', index = False)\n",
        "        \n",
        "        numLexTokens.append(len(score))\n",
        "        \n",
        "        if len(score) != 0: average_score = sum(score) / len(score)\n",
        "        else: average_score = 0.5\n",
        "        avgLexVal.append(average_score)\n",
        "            \n",
        "    # 파일 저장 확인\n",
        "    result_DF = pd.DataFrame((zip(numTokens, numLexTokens, avgLexVal)), columns = ['numTokens', 'numLexTokens', 'avgLexVal'])\n",
        "    result_DF.to_csv(save_path, mode='w', index = False)\n",
        "    \n",
        "    return result_DF \n",
        "\n",
        "# 1000개 게시글 처리하는데 걸리는 시간 약 10-12분"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a326d59"
      },
      "outputs": [],
      "source": [
        "## 점수 매기는 과정 (without non mental health)\n",
        "\n",
        "dep_val_score = score_by_lex_original(lexicon_Val, dep_cleansing,'./scoring_result/depression_valence_score.csv')\n",
        "dep_aro_score = score_by_lex_original(lexicon_Aro, dep_cleansing,'./scoring_result/depression_arousal_score.csv')\n",
        "\n",
        "bipolar_val_score = score_by_lex_original(lexicon_Val, bipolar_cleansing,'./scoring_result/depression_valence_score.csv')\n",
        "bipolar_aro_score = score_by_lex_original(lexicon_Aro, bipolar_cleansing,'./scoring_result/depression_arousal_score.csv')\n",
        "\n",
        "panic_val_score = score_by_lex_original(lexicon_Val, anxiety_cleansing,'./scoring_result/depression_valence_score.csv')\n",
        "panic_aro_score = score_by_lex_original(lexicon_Aro, anxiety_cleansing,'./scoring_result/depression_arousal_score.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3888321b"
      },
      "outputs": [],
      "source": [
        "## 결과 출력\n",
        "print(\"DEPRESSION\")\n",
        "dep_val_score = pd.read_csv(\"./scoring_result/depression_valence_score.csv\")\n",
        "print(\"V : \",sum(dep_val_score)/len(dep_val_score))\n",
        "print(\"A : \",sum(dep_aro_score)/len(dep_aro_score))\n",
        "\n",
        "print(\"BIPOLAR\")\n",
        "bipolar_val_score = pd.read_csv(\"./scoring_result/bipolar_valence_score.csv\")\n",
        "print(\"V : \",sum(bipolar_val_score)/len(bipolar_val_score))\n",
        "print(\"A : \",sum(bipolar_aro_score)/len(bipolar_aro_score))\n",
        "\n",
        "print(\"PANIC\")\n",
        "panic_val_score = pd.read_csv(\"./scoring_result/panic_valence_score.csv\")\n",
        "print(\"V : \",sum(panic_val_score)/len(panic_val_score))\n",
        "print(\"A : \",sum(panic_aro_score)/len(panic_aro_score))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 수정한 알고리즘"
      ],
      "metadata": {
        "id": "8bzu4px-1ZA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 구성 변경> 사전 전처리"
      ],
      "metadata": {
        "id": "8-RQ-o2n7Crt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon_Val = pd.read_csv(\"./Val_lex_Zscore.csv\", sep = ',')\n",
        "lexicon_Val.columns =['Emotion','Score']\n",
        "lexicon_Val=lexicon_Val.dropna()\n",
        "lexicon_Val = lexicon_Val.reset_index()\n",
        "lexicon_Val = lexicon_Val.drop(columns = ['index'])\n",
        "\n",
        "lexicon_Aro = pd.read_csv(\"./Aro_lex_Zscore.csv\", sep = ',')\n",
        "lexicon_Aro.columns =['Emotion','Score']\n",
        "lexicon_Aro=lexicon_Aro.dropna()\n",
        "lexicon_Aro = lexicon_Aro.reset_index()\n",
        "lexicon_Aro = lexicon_Aro.drop(columns = ['index'])\n"
      ],
      "metadata": {
        "id": "9aqRumtUJLI4"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B_INCR = 0.293\n",
        "B_DECR = -0.293\n",
        "\n",
        "C_INCR = 0.733\n",
        "N_SCALAR = -0.74\n",
        "\n",
        "NEGATE = \\\n",
        "    [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\",\n",
        "     \"ain't\", \"aren't\", \"can't\", \"couldn't\", \"daren't\", \"didn't\", \"doesn't\",\n",
        "     \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\", \"neither\",\n",
        "     \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\",\n",
        "     \"neednt\", \"needn't\", \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\",\n",
        "     \"oughtnt\", \"shant\", \"shouldnt\", \"uhuh\", \"wasnt\", \"werent\",\n",
        "     \"oughtn't\", \"shan't\", \"shouldn't\", \"uh-uh\", \"wasn't\", \"weren't\",\n",
        "     \"without\", \"wont\", \"wouldnt\", \"won't\", \"wouldn't\", \"rarely\", \"seldom\", \"despite\"]\n",
        "\n",
        "# http://en.wiktionary.org/wiki/Category:English_degree_adverbs\n",
        "\n",
        "BOOSTER_DICT = \\\n",
        "    {\"absolutely\": B_INCR, \"amazingly\": B_INCR, \"awfully\": B_INCR,\n",
        "     \"completely\": B_INCR, \"considerable\": B_INCR, \"considerably\": B_INCR,\n",
        "     \"decidedly\": B_INCR, \"deeply\": B_INCR, \"effing\": B_INCR, \"enormous\": B_INCR, \"enormously\": B_INCR,\n",
        "     \"entirely\": B_INCR, \"especially\": B_INCR, \"exceptional\": B_INCR, \"exceptionally\": B_INCR,\n",
        "     \"extreme\": B_INCR, \"extremely\": B_INCR,\n",
        "     \"fabulously\": B_INCR, \"flipping\": B_INCR, \"flippin\": B_INCR, \"frackin\": B_INCR, \"fracking\": B_INCR,\n",
        "     \"fricking\": B_INCR, \"frickin\": B_INCR, \"frigging\": B_INCR, \"friggin\": B_INCR, \"fully\": B_INCR,\n",
        "     \"fuckin\": B_INCR, \"fucking\": B_INCR, \"fuggin\": B_INCR, \"fugging\": B_INCR,\n",
        "     \"greatly\": B_INCR, \"hella\": B_INCR, \"highly\": B_INCR, \"hugely\": B_INCR,\n",
        "     \"incredible\": B_INCR, \"incredibly\": B_INCR, \"intensely\": B_INCR,\n",
        "     \"major\": B_INCR, \"majorly\": B_INCR, \"more\": B_INCR, \"most\": B_INCR, \"particularly\": B_INCR,\n",
        "     \"purely\": B_INCR, \"quite\": B_INCR, \"really\": B_INCR, \"remarkably\": B_INCR,\n",
        "     \"so\": B_INCR, \"substantially\": B_INCR,\n",
        "     \"thoroughly\": B_INCR, \"total\": B_INCR, \"totally\": B_INCR, \"tremendous\": B_INCR, \"tremendously\": B_INCR,\n",
        "     \"uber\": B_INCR, \"unbelievably\": B_INCR, \"unusually\": B_INCR, \"utter\": B_INCR, \"utterly\": B_INCR,\n",
        "     \"very\": B_INCR,\n",
        "     \"almost\": B_DECR, \"barely\": B_DECR, \"hardly\": B_DECR, \"just enough\": B_DECR,\n",
        "     \"kind of\": B_DECR, \"kinda\": B_DECR, \"kindof\": B_DECR, \"kind-of\": B_DECR,\n",
        "     \"less\": B_DECR, \"little\": B_DECR, \"marginal\": B_DECR, \"marginally\": B_DECR,\n",
        "     \"occasional\": B_DECR, \"occasionally\": B_DECR, \"partly\": B_DECR,\n",
        "     \"scarce\": B_DECR, \"scarcely\": B_DECR, \"slight\": B_DECR, \"slightly\": B_DECR, \"somewhat\": B_DECR,\n",
        "     \"sort of\": B_DECR, \"sorta\": B_DECR, \"sortof\": B_DECR, \"sort-of\": B_DECR}\n"
      ],
      "metadata": {
        "id": "nzj3qIqj5YH-"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "scrolled": true,
        "id": "edbf3c2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8474160a-3d8e-4136-8809-fee26c9ca97f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'hed', 'very', 'gone', 'biol', 'that', 'throughout', 'sure', 'seems', 'shouldn', 're', 'won', 'ex', 'affected', 'may', 'his', 'most', '}', 'u', 'let', 'resulted', 'ff', 'must', 'by', 'effect', 'obtained', 'beyond', 'nowhere', \"it's\", 'too', 'while', 'affects', 'id', 'specifying', 'largely', 'somebody', 'announce', 'itd', 'quickly', 'kg', 'to', 'whole', 'down', 'cannot', 'useful', 'us', 'have', 'shan', 'recently', 'contain', 'looking', 'is', 'normally', 'several', 'significantly', \"i've\", 'mg', 'welcome', 'auth', 'im', 'use', 'til', 'namely', 'does', 'placed', 'thank', 'inc', 'thereto', 'ending', 'sufficiently', 'words', 'home', 'co', 'shown', 'again', 'index', 'sometimes', 'apparently', 'whereas', \"weren't\", 'viz', 'keeps', 'following', 'next', 'okay', 'saw', 'means', \"you'll\", 'aren', 'km', 'particularly', 'hardly', 'took', 'nine', '\\\\', 'wheres', 'certainly', 'm', 'latter', 'later', 'sec', 'ie', 'slightly', 'happens', 'theirs', 'any', 'part', 'r', 'end', 'anyways', 'except', 'did', 'through', 'other', 'give', 'up', 'ought', 'please', \"we'll\", 'knows', 'many', 'them', 'in', ']', 'l', 'x', 'c', 'selves', 'e', 'shed', 'via', 'near', 'even', 'couldnt', 'with', 'million', 'seeing', 'theyd', 'am', 'gave', 'nay', 'their', 'couldn', 'fix', 'whence', 'ask', 'haven', 'of', 'aside', 'mostly', 'otherwise', 'become', 'went', 'don', 'asking', 'try', 'wherein', 'potentially', \"couldn't\", 'least', 'immediately', 'nothing', 'every', '=', 'me', 'thered', 'thou', 'says', \"there've\", 'although', 'seven', 'mr', 'about', 'beforehand', 'showed', 'willing', 'what', 'which', 'been', 'followed', 'present', 'therere', '?', 'usefulness', 'who', 'strongly', 'qv', 'just', 'abst', 'vol', 'b', ':', 'www', 'anything', 'on', 'importance', 'nevertheless', 'theyre', 'shows', 'think', 'if', 'eight', '~', 'added', 'only', 'would', 'due', 'for', 'former', 'yourselves', 'onto', 'whom', 'ran', '<', 'furthermore', 'further', 'already', 'see', 'seeming', 'often', 'hadn', 'but', 'according', 'want', 'herself', 'value', 'itself', 'ups', 'necessarily', 'six', '.', 'old', 'few', 'no', \"you're\", 'both', 'wed', \"doesn't\", 'go', 'page', 'know', 'right', 'hasn', 'needs', 'our', 'doing', 'during', 'sent', \"mustn't\", 'noone', 'ed', 'alone', \"'ve\", 'zero', 'mustn', 'regards', 'whereby', 'ever', 'specify', \"needn't\", 's', 'hers', 'moreover', 'vs', 'yes', 'at', 'seemed', 'unless', 'behind', 'until', 'toward', 'yourself', 'whether', 'trying', 'taking', 'are', 'section', 'uses', 'werent', 'didn', 'they', 'using', 'h', 'j', 'awfully', 'specifically', 've', 'afterwards', 'research', 'wasn', 'all', 'together', 'around', \"don't\", 'regarding', 'weren', 'eg', \"aren't\", 'hither', 'wish', 'omitted', 'sorry', 'kept', 'this', 'downwards', 'different', \"that'll\", 'her', 'whereupon', '[', 'might', 'recent', 'myself', 'same', 'way', 'widely', 'made', 'found', 'et', 'et-al', 'unto', 'approximately', 'new', 'nonetheless', 'within', '#', 'particular', 'above', 'yet', 'shes', 'forth', \"we've\", 'anybody', 'begins', 'ours', 'nos', \"she'll\", 'truly', 'has', 'possible', 'invention', ')', 'suggest', 'youre', 'youd', ',', 'never', 'obtain', 'keep', 'herein', '%', 'hundred', 'miss', 'd', 'f', 'gotten', 'somehow', 'line', 'enough', 'doesn', 'gets', 'brief', 'whats', 'taken', 'accordingly', 'had', 'little', 'thanx', 'ts', 'anyhow', 'wouldnt', 'pp', 'somewhere', 'mainly', 'oh', 'now', 'per', 'thru', 'tip', 'latterly', 'where', 'himself', 'k', 'contains', 'anyway', 'usually', 'vols', 'not', 'pages', 'fifth', 'ca', 'out', 'refs', 'each', 'far', 'wants', 'a', \"shan't\", \"they'll\", 'meantime', '-', 'available', 'something', \"won't\", 'nobody', 'anymore', 'she', \"'ll\", 'especially', 'such', 'thus', 'wasnt', 'one', 'showns', 'beside', 'un', 'another', 'given', 'own', 'shall', 'thereafter', 'instead', \"can't\", 'yours', 'gives', 'across', 'comes', \"who'll\", 'really', 'arent', 'your', 'over', 'formerly', 'became', 'results', 'whereafter', \"that've\", 'known', 'put', \"hadn't\", \"they've\", 'upon', 'actually', 'having', 'whatever', 'always', 'whenever', 'need', 'or', 'after', 'lest', 'g', 'v', 'getting', 'beginning', 'hi', 'owing', 'whim', \"wouldn't\", 'almost', 'however', \"wasn't\", 'p', 'also', 'towards', 'everyone', 'themselves', 'significant', 'act', \"she's\", '&', 'needn', 'wont', 'necessary', 'hereby', 'whose', '$', 'plus', 'an', 'still', 'hes', 'q', 'becomes', 'thereby', 'do', 'ord', 'seem', '*', 'here', 'inward', 'either', 'four', 'ones', '{', 'some', 'respectively', 'say', \"what'll\", 'last', 'lets', 'how', 'perhaps', 'there', 'neither', 'ain', 'previously', 'anywhere', 'able', 'important', 'que', 'anyone', 'com', 'away', 'though', 'certain', 'isn', 'before', 'n', 'thence', 'much', 'thereupon', 'tried', 'goes', 'along', 'make', 'sometime', 'll', 'amongst', 'proud', 'whomever', 'resulting', 'like', 'unlike', 'predominantly', 'relatively', '^', 'information', 'soon', 'elsewhere', 'hereupon', 'take', 'mrs', 'five', 'self', 'readily', 'tell', 'stop', '`', 'ah', 'whither', 'mean', 'look', 'causes', 'tries', 'outside', '>', 'obviously', 'likely', \"it'll\", \"you've\", 'than', 'more', 'less', 'thoughh', \"haven't\", 'among', 'promptly', ';', \"shouldn't\", 'believe', \"didn't\", 'whoever', 'unlikely', 'without', 'indeed', 'mightn', 'usefully', 'else', 'containing', 'i', 'seen', 'first', 'non', 'poorly', 'mug', 'thats', 'ma', \"there'll\", 'because', 'heres', 'said', 'briefly', 'successfully', 'we', 'ref', 'the', 'none', 'being', 'once', 'into', 'everything', 'come', 'possibly', 'ltd', 'as', 'nearly', \"should've\", 'wouldn', 'were', 'since', 'related', 'could', 'should', 'my', 'done', 'used', 'under', 'follows', 'those', '@', \"isn't\", 'so', 'be', 'somethan', 'someone', 'similarly', 'primarily', 'z', 'lately', 'two', 'world', 'back', \"mightn't\", 'substantially', 'rather', 'similar', 'below', 'therein', \"you'd\", 'from', 'y', 'hence', 'arise', '/', 'ml', 'run', '|', 'then', 'affecting', 'against', 'unfortunately', 'makes', 'eighty', 'howbeit', 'immediate', 'th', 'liked', 'o', 'these', 'and', 'maybe', 'get', 'probably', 'accordance', \"'\", 'besides', 'theres', 'can', 'sub', 'he', 'merely', 'nd', 'regardless', '!', 'everybody', 'wherever', 'date', 'etc', 'nor', 'cause', 'rd', 'tends', 'noted', 'it', 'its', 'becoming', 'therefore', 'edu', 'thanks', 'got', 'ok', 'will', 'hereafter', 'show', 'came', 'between', 'hid', 'was', 'when', '_', 'adj', 'you', 'ourselves', 'provides', '+', 'sup', 'twice', '(', 'name', 'him', '\"', 'meanwhile', 'overall', 'w', 'others', 'saying', \"i'll\", 't', 'looks', 'ninety', 'specified', 'whod', 'giving', 'na', 'whos', 'throug', 'quite', 'beginnings', 'various', \"hasn't\", 'why', 'thousand', 'off', 'thereof', 'begin', 'somewhat', 'past', 'everywhere'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "## 불용어 정의 \n",
        "nltk.download('stopwords')\n",
        "stop_words = list(set(stopwords.words('english')))\n",
        "\n",
        "STOP_WORDS = set()\n",
        "\n",
        "file = open(\"./stopwords.txt\", \"r\")\n",
        "while True:\n",
        "    line = file.readline()\n",
        "    if not line:\n",
        "        break\n",
        "    STOP_WORDS.add(line.strip())\n",
        "\n",
        "file.close()\n",
        "len(STOP_WORDS)\n",
        "\n",
        "for word in stop_words:\n",
        "    if word not in STOP_WORDS:\n",
        "        STOP_WORDS.add(word)\n",
        "print(STOP_WORDS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "39227bb7"
      },
      "outputs": [],
      "source": [
        "## 부정 표현, 부사어 제거\n",
        "\n",
        "for neg in NEGATE: \n",
        "    if neg in STOP_WORDS: STOP_WORDS.remove(neg)\n",
        "for booster in BOOSTER_DICT: \n",
        "    if booster in STOP_WORDS: STOP_WORDS.remove(booster)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(lexicon_Val)):\n",
        "    if lexicon_Val['Emotion'][i] in BOOSTER_DICT:\n",
        "        lexicon_Val.drop(i,inplace=True)\n",
        "        lexicon_Aro.drop(i,inplace=True)\n",
        "\n",
        "lexicon_Val=lexicon_Val.reset_index(drop=True)\n",
        "lexicon_Val=lexicon_Val.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "edZFdIdomTNS"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon_Val[lexicon_Val['Emotion']=='extremely']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "sKy8iNOwQ2Lm",
        "outputId": "3d329672-efe5-44bf-e2ec-91ac3d714dd2"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [Emotion, Score]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c7d45df-3c44-445e-a219-133f1403f07b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c7d45df-3c44-445e-a219-133f1403f07b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1c7d45df-3c44-445e-a219-133f1403f07b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1c7d45df-3c44-445e-a219-133f1403f07b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 알고리즘 구현"
      ],
      "metadata": {
        "id": "nLdpyYGA7T1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def booster_score_update(token, emo_score):\n",
        "    if emo_score < 0:\n",
        "        emo_score = emo_score - BOOSTER_DICT[token]\n",
        "    else: \n",
        "        emo_score = emo_score + BOOSTER_DICT[token]\n",
        "    \n",
        "    return emo_score\n",
        "\n",
        "def negate_score_update(emo_score):\n",
        "    return emo_score * N_SCALAR\n",
        "\n",
        "def word_similarity(sim_word_lst, word):\n",
        "    similar_lst = []\n",
        "    for w in sim_word_lst:\n",
        "        similar_lst.append(len(w))\n",
        "    \n",
        "    return np.argmax(similar_lst)  \n",
        "      "
      ],
      "metadata": {
        "id": "-e9TnNzXaEnb"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def score_by_lex_upgrade_together(lexicon_val,lexicon_aro, text_df, save_path):#lexicon scoring algorithm\n",
        "    numTokens = []\n",
        "    numTokens_without_stop = []\n",
        "    numLexTokens = []\n",
        "    avgLexVal = []\n",
        "    avgLexAro = []\n",
        "    check = 0\n",
        "    for t in text_df:\n",
        "        \n",
        "        score_lstV = []\n",
        "        score_lstA = []\n",
        "        text = str(t)\n",
        "        tokenized = text_to_word_sequence(text)\n",
        "        numTokens.append(len(tokenized))\n",
        "        \n",
        "        ## 불용어 제거\n",
        "        token_without_stop = []\n",
        "        for token in tokenized:\n",
        "            if token not in STOP_WORDS:\n",
        "                token_without_stop.append(token)\n",
        "        numTokens_without_stop.append(len(token_without_stop))    \n",
        "        \n",
        "        tok_index = 0\n",
        "        for token in token_without_stop:\n",
        "            emo_index = 0\n",
        "            emoWord_lst = []\n",
        "            tempV = []\n",
        "            tempA = []\n",
        "            for emotion in lexicon_val['Emotion']:\n",
        "                if emotion in token:\n",
        "                    emoWord_lst.append(emotion)\n",
        "                    ## 부정표현 & 부사 처리 알고리즘\n",
        "                    val_score = lexicon_val['Score'][emo_index]\n",
        "                    aro_score = lexicon_aro['Score'][emo_index]\n",
        "                    check_index = tok_index - 1\n",
        "                    while check_index >= 0:\n",
        "                        if token_without_stop[check_index] in NEGATE:\n",
        "                            val_score = negate_score_update(val_score)\n",
        "                            aro_score = negate_score_update(aro_score)\n",
        "                            break\n",
        "                        elif token_without_stop[check_index] in BOOSTER_DICT:\n",
        "                            val_score = booster_score_update(token_without_stop[check_index], val_score)\n",
        "                            aro_score = booster_score_update(token_without_stop[check_index], aro_score)\n",
        "                            check_index -= 1\n",
        "                            continue\n",
        "                        elif nltk.pos_tag([token_without_stop[check_index]])[0][1] in {'RB','RBR','RBS'}:\n",
        "                            check_index -= 1\n",
        "                            continue\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    tempV.append(val_score)\n",
        "                    tempA.append(aro_score)\n",
        "\n",
        "                emo_index += 1\n",
        "            tok_index += 1 \n",
        "\n",
        "            if len(emoWord_lst) != 0:\n",
        "                score_lstV.append(tempV[word_similarity(emoWord_lst,token)])\n",
        "                score_lstA.append(tempA[word_similarity(emoWord_lst,token)])   \n",
        "\n",
        "        check += 1\n",
        "        if check % 1000 == 0: # 중간 저장 과정\n",
        "            print(check)  \n",
        "            result_DF = pd.DataFrame((zip(numTokens, numLexTokens, avgLexVal, avgLexAro)), columns = ['numTokens', 'numLexTokens', 'avgLexVal', 'avgLexAro'])\n",
        "            result_DF.to_csv(save_path, mode='w', index = False)\n",
        "        \n",
        "        numLexTokens.append(len(score_lstV))\n",
        "        # print(score_lstV)\n",
        "        if len(score_lstV) != 0: \n",
        "            average_score = [sum(score_lstV) / len(score_lstV), sum(score_lstA) / len(score_lstA)]\n",
        "        else: average_score = [0,0]\n",
        "        avgLexVal.append(average_score[0])\n",
        "        avgLexAro.append(average_score[1])\n",
        "\n",
        "    \n",
        "    # 파일 저장 확인\n",
        "    result_DF = pd.DataFrame((zip(numTokens, numLexTokens, avgLexVal, avgLexAro)), columns = ['numTokens', 'numLexTokens', 'avgLexVal', 'avgLexAro'])\n",
        "    result_DF.to_csv(save_path, mode='w', index = False)\n",
        "    \n",
        "    return result_DF\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAyaAV8muoUd",
        "outputId": "9986a98e-e83b-44d8-b06a-fa1a5609f65c"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check by sample"
      ],
      "metadata": {
        "id": "A42DgI537d1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_lst = []\n",
        "sample_lst.append('Im in bad mood')\n",
        "sample_lst.append('Im in extremely bad mood')\n",
        "sample_lst.append('Im not in extremely bad mood')\n",
        "\n",
        "sample_lst.append('I feel so gross')\n",
        "sample_lst.append('I feel gross')\n",
        "sample_lst.append('I dont feel so gross')\n",
        "\n",
        "sample_lst.append('The weather was magnificent.')\n",
        "sample_lst.append('The weather was kinda magnificent.')\n",
        "\n",
        "sample_lst.append('I am sick. I\\'m not really good.')\n",
        "sample_lst.append('I\\'m not really really good')\n",
        "sample_lst.append('I\\'m not really good')\n",
        "\n",
        "sample_lst.append('It was tremendous war')\n",
        "sample_lst.append('It was sort of tremendous war')\n",
        "sample_lst.append('It was not tremendous war')\n",
        "sample_lst.append('It was actually tremendous war')\n",
        "sample_lst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3O3bY7yPm8b",
        "outputId": "6da8f405-6494-4d5d-e7be-1c5a9bc5ec29"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Im in bad mood',\n",
              " 'Im in extremely bad mood',\n",
              " 'Im not in extremely bad mood',\n",
              " 'I feel so gross',\n",
              " 'I feel gross',\n",
              " 'I dont feel so gross',\n",
              " 'The weather was magnificent.',\n",
              " 'The weather was kinda magnificent.',\n",
              " \"I am sick. I'm not really good.\",\n",
              " \"I'm not really really good\",\n",
              " \"I'm not really good\",\n",
              " 'It was tremendous war',\n",
              " 'It was sort of tremendous war',\n",
              " 'It was not tremendous war',\n",
              " 'It was actually tremendous war']"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_score_up = score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, sample_lst,'./scoring_result/sample_score.csv')"
      ],
      "metadata": {
        "id": "Q3yVIcmpSggn"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_score_up['text'] = sample_lst\n",
        "sample_score_up\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "MMRUOy-0STBV",
        "outputId": "fca1e7a0-03cb-4654-d022-b0f41f16f1ab"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    numTokens  numLexTokens  avgLexVal  avgLexAro  \\\n",
              "0           4             2  -0.359324  -0.204329   \n",
              "1           5             3  -0.723604  -0.565070   \n",
              "2           6             3   1.127132   1.028650   \n",
              "3           4             2  -0.338900  -0.102370   \n",
              "4           3             2  -0.192400   0.044130   \n",
              "5           5             3  -0.812459  -0.777254   \n",
              "6           4             2   1.750402   2.017188   \n",
              "7           5             3   1.715332   2.016956   \n",
              "8           7             3  -1.519256  -1.554889   \n",
              "9           5             3  -1.385735  -1.550915   \n",
              "10          4             2  -1.360818  -1.557868   \n",
              "11          4             2  -1.157288  -1.198514   \n",
              "12          6             3  -0.504131  -0.543635   \n",
              "13          5             2   0.856393   0.886900   \n",
              "14          5             2  -1.157288  -1.198514   \n",
              "\n",
              "                                  text  \n",
              "0                       Im in bad mood  \n",
              "1             Im in extremely bad mood  \n",
              "2         Im not in extremely bad mood  \n",
              "3                      I feel so gross  \n",
              "4                         I feel gross  \n",
              "5                 I dont feel so gross  \n",
              "6         The weather was magnificent.  \n",
              "7   The weather was kinda magnificent.  \n",
              "8      I am sick. I'm not really good.  \n",
              "9           I'm not really really good  \n",
              "10                 I'm not really good  \n",
              "11               It was tremendous war  \n",
              "12       It was sort of tremendous war  \n",
              "13           It was not tremendous war  \n",
              "14      It was actually tremendous war  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-507ec620-3b79-4512-a6b2-351d24b70370\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>numTokens</th>\n",
              "      <th>numLexTokens</th>\n",
              "      <th>avgLexVal</th>\n",
              "      <th>avgLexAro</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.359324</td>\n",
              "      <td>-0.204329</td>\n",
              "      <td>Im in bad mood</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.723604</td>\n",
              "      <td>-0.565070</td>\n",
              "      <td>Im in extremely bad mood</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>1.127132</td>\n",
              "      <td>1.028650</td>\n",
              "      <td>Im not in extremely bad mood</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.338900</td>\n",
              "      <td>-0.102370</td>\n",
              "      <td>I feel so gross</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.192400</td>\n",
              "      <td>0.044130</td>\n",
              "      <td>I feel gross</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.812459</td>\n",
              "      <td>-0.777254</td>\n",
              "      <td>I dont feel so gross</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1.750402</td>\n",
              "      <td>2.017188</td>\n",
              "      <td>The weather was magnificent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>1.715332</td>\n",
              "      <td>2.016956</td>\n",
              "      <td>The weather was kinda magnificent.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>-1.519256</td>\n",
              "      <td>-1.554889</td>\n",
              "      <td>I am sick. I'm not really good.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>-1.385735</td>\n",
              "      <td>-1.550915</td>\n",
              "      <td>I'm not really really good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.360818</td>\n",
              "      <td>-1.557868</td>\n",
              "      <td>I'm not really good</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.157288</td>\n",
              "      <td>-1.198514</td>\n",
              "      <td>It was tremendous war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.504131</td>\n",
              "      <td>-0.543635</td>\n",
              "      <td>It was sort of tremendous war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0.856393</td>\n",
              "      <td>0.886900</td>\n",
              "      <td>It was not tremendous war</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>-1.157288</td>\n",
              "      <td>-1.198514</td>\n",
              "      <td>It was actually tremendous war</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-507ec620-3b79-4512-a6b2-351d24b70370')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-507ec620-3b79-4512-a6b2-351d24b70370 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-507ec620-3b79-4512-a6b2-351d24b70370');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_score_origV = score_by_lex_original(lexicon_Val, sample_lst,'./scoring_result/upgraded_twice/sample_score_OV.csv')\n",
        "sample_score_origA = score_by_lex_original(lexicon_Aro, sample_lst,'./scoring_result/upgraded_twice/sample_score_OV.csv')"
      ],
      "metadata": {
        "id": "2hh35U4TW1Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_score_origA.columns = ['numTokens', 'numLexTokens', 'avgLexAro']\n",
        "sample_score_orig =pd.concat([sample_score_origV, sample_score_origA[['avgLexAro']]],axis=1)\n",
        "sample_score_orig['text'] = sample_lst\n",
        "sample_score_orig"
      ],
      "metadata": {
        "id": "Y61b_hRoXPJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 적용"
      ],
      "metadata": {
        "id": "3MGa573k7u08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dep_cleansing = pd.read_csv(\"./cleansing_dep.csv\")\n",
        "bipolar_cleansing = pd.read_csv(\"./cleansing_bip.csv\")\n",
        "anxiety_cleansing = pd.read_csv(\"./cleansing_pan.csv\")\n",
        "\n",
        "dep_cleansing = dep_cleansing['0']\n",
        "bipolar_cleansing = bipolar_cleansing['0']\n",
        "anxiety_cleansing = anxiety_cleansing['0']\n",
        "\n",
        "covid_cleansing = pd.read_csv(\"./cleansing_cov.csv\")\n",
        "# relationship_cleansing = pd.read_csv(\"./cleansing_rel.csv\")\n",
        "teaching_cleansing = pd.read_csv(\"./cleansing_tea.csv\")\n",
        "\n",
        "covid_cleansing = covid_cleansing['0']\n",
        "# relationship_cleansing = relationship_cleansing['0']\n",
        "teaching_cleansing = teaching_cleansing['0']"
      ],
      "metadata": {
        "id": "A9Gq_yOPvDn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depression_score = score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, dep_cleansing,'./scoring_result/upgraded_twice/depression_score.csv')\n",
        "\n",
        "bipolar_score = score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, bipolar_cleansing,'./scoring_result/upgraded_twice/bipolar_score.csv')\n",
        "\n",
        "panic_score =  score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, anxiety_cleansing,'./scoring_result/upgraded_twice/panic_score.csv')\n",
        "\n",
        "covid_score = score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, covid_cleansing,'./scoring_result/upgraded_twice/covid_score.csv')\n",
        "\n",
        "teaching_score = score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, teaching_cleansing,'./scoring_result/upgraded_twice/teaching_score.csv')"
      ],
      "metadata": {
        "id": "4HwQX8NH54M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Category Intensity Scoring Algorithm"
      ],
      "metadata": {
        "id": "UwbX9npSI6C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## category lexicon 불러오기\n"
      ],
      "metadata": {
        "id": "RpwNFHdtJBvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## category lexicon 불러오기\n",
        "lexicon_Cat = pd.read_csv(\"./Intensity_lexicon_original.csv\", sep = ',')\n",
        "lexicon_Cat.columns = ['word','anger','anticipation','disgust','fear','joy','sadness','surprise','trust']\n",
        "lexicon_Cat = lexicon_Cat.dropna()\n",
        "lexicon_Cat = lexicon_Cat.reset_index()\n",
        "lexicon_Cat = lexicon_Cat.drop(columns = ['index'])"
      ],
      "metadata": {
        "id": "W_gyPgNPI9rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reddit Text dataset 불러오기"
      ],
      "metadata": {
        "id": "IliSpUXMJGyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dep_cleansing = pd.read_csv(\"./cleansing_dep.csv\")\n",
        "bipolar_cleansing = pd.read_csv(\"./cleansing_bip.csv\")\n",
        "anxiety_cleansing = pd.read_csv(\"./cleansing_pan.csv\")\n",
        "\n",
        "dep_cleansing = dep_cleansing['0']\n",
        "bipolar_cleansing = bipolar_cleansing['0']\n",
        "anxiety_cleansing = anxiety_cleansing['0']"
      ],
      "metadata": {
        "id": "KcUuVFi5JNac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covid_cleansing = pd.read_csv(\"./cleansing_cov.csv\")\n",
        "relationship_cleansing = pd.read_csv(\"./cleansing_rel.csv\")\n",
        "teaching_cleansing = pd.read_csv(\"./cleansing_tea.csv\")\n",
        "\n",
        "covid_cleansing = covid_cleansing['0']\n",
        "relationship_cleansing = relationship_cleansing['0']\n",
        "teaching_cleansing = teaching_cleansing['0']"
      ],
      "metadata": {
        "id": "zhqqSI9IJYc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# date 정보 포함 데이터셋\n",
        "\n",
        "depression = pd.read_csv(\"./psy_data/year_month_col/depress_ym.csv\")\n",
        "bipolar = pd.read_csv(\"./psy_data/year_month_col/bipolar_ym.csv\")\n",
        "panic = pd.read_csv(\"./psy_data/year_month_col/panic_ym.csv\")\n"
      ],
      "metadata": {
        "id": "KY0h0cxFJZuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covid = pd.read_csv(\"./psy_data/year_month_col/covid_ym.csv\")\n",
        "relationship = pd.read_csv(\"./psy_data/year_month_col/relationship_ym.csv\")\n",
        "teaching = pd.read_csv(\"./psy_data/year_month_col/teaching_ym.csv\")"
      ],
      "metadata": {
        "id": "jx-xCCCYJbLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Category Check Algorithm"
      ],
      "metadata": {
        "id": "z8VmErmuJGwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 별도로 normalize 필요함\n",
        "def category_scoring(lexicon, clean_lst, text_df, save_path):\n",
        "    if len(clean_lst) != len(text_df):\n",
        "        print(\"wrong dataset\")\n",
        "        return\n",
        "    \n",
        "    cat_result = text_df[['date', 'year', 'month']]\n",
        "    # word','anger','anticipation','disgust','fear','joy','sadness','surprise','trust']\n",
        "    score_matrix = np.zeros((len(clean_lst), 8), dtype=float)\n",
        "\n",
        "    check = 0\n",
        "    for t in clean_lst:\n",
        "        text = str(t)\n",
        "        text_score = np.zeros(8,dtype=float)\n",
        "        \n",
        "        for index in range(len(lexicon)):\n",
        "            emotion = lexicon['word'][index]\n",
        "            row_score = np.array(lexicon.iloc[index][1:], dtype=float)\n",
        "            count = text.count(emotion)\n",
        "            \n",
        "            row_score *= count\n",
        "            text_score += row_score\n",
        "        \n",
        "        score_matrix[check] = text_score\n",
        "        check += 1\n",
        "        if check % 3000 == 0:\n",
        "            print(check)  \n",
        "            temp = pd.DataFrame(score_matrix, columns=['anger','anticipation','disgust','fear','joy','sadness','surprise','trust'])\n",
        "            result_DF = pd.concat([cat_result, temp], axis=1)\n",
        "            result_DF.to_csv(save_path, mode='w', index = False)\n",
        "\n",
        "        # check += 1\n",
        "    \n",
        "\n",
        "    temp = pd.DataFrame(score_matrix, columns=['anger','anticipation','disgust','fear','joy','sadness','surprise','trust'])\n",
        "    result_DF = pd.concat([cat_result, temp], axis=1)\n",
        "    result_DF.to_csv(save_path, mode='w', index = False)"
      ],
      "metadata": {
        "id": "578tvieGJeSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "category_scoring(lexicon_Cat, dep_cleansing, depression, './scoring_result/category/depression_catScore.csv')\n",
        "\n",
        "category_scoring(lexicon_Cat, bipolar_cleansing, bipolar, './scoring_result/category/bipolar_catScore.csv')\n",
        "\n",
        "category_scoring(lexicon_Cat, anxiety_cleansing, panic, './scoring_result/category/panic_catScore.csv')\n",
        "\n",
        "category_scoring(lexicon_Cat, covid_cleansing, covid, './scoring_result/category/covid_catScore.csv')\n",
        "\n",
        "category_scoring(lexicon_Cat, teaching_cleansing, teaching, './scoring_result/category/teaching_catScore.csv')"
      ],
      "metadata": {
        "id": "YE3mpbrPJmq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "toGgipIRYyHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VA Scoring Neg/Pos Independently"
      ],
      "metadata": {
        "id": "eNEABEiFTWfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def score_by_lex_upgrade_NPInd(lexicon_val,lexicon_aro, text_df, save_path):#lexicon scoring algorithm\n",
        "    numTokens = []\n",
        "    numTokens_without_stop = []\n",
        "    numPVTokens = []\n",
        "    numNVTokens = []\n",
        "    numPATokens = []\n",
        "    numNATokens = []\n",
        "    avgLexPVal = []\n",
        "    avgLexNVal = []\n",
        "    avgLexPAro = []\n",
        "    avgLexNAro = []\n",
        "    check = 0\n",
        "    for t in text_df:\n",
        "        \n",
        "        score_lstV = []\n",
        "        score_lstA = []\n",
        "        text = str(t)\n",
        "        tokenized = text_to_word_sequence(text)\n",
        "        numTokens.append(len(tokenized))\n",
        "        \n",
        "        ## 불용어 제거\n",
        "        token_without_stop = []\n",
        "        for token in tokenized:\n",
        "            if token not in STOP_WORDS:\n",
        "                token_without_stop.append(token)\n",
        "        numTokens_without_stop.append(len(token_without_stop))    \n",
        "        \n",
        "        tok_index = 0\n",
        "        for token in token_without_stop:\n",
        "            emo_index = 0\n",
        "            emoWord_lst = []\n",
        "            tempV = []\n",
        "            tempA = []\n",
        "            for emotion in lexicon_val['Emotion']:\n",
        "                if emotion in token:\n",
        "                    emoWord_lst.append(emotion)\n",
        "                    ## 부정표현 & 부사 처리 알고리즘\n",
        "                    val_score = lexicon_val['Score'][emo_index]\n",
        "                    aro_score = lexicon_aro['Score'][emo_index]\n",
        "                    check_index = tok_index - 1\n",
        "                    while check_index >= 0:\n",
        "                        if token_without_stop[check_index] in NEGATE:\n",
        "                            val_score = negate_score_update(val_score)\n",
        "                            aro_score = negate_score_update(aro_score)\n",
        "                            break\n",
        "                        elif token_without_stop[check_index] in BOOSTER_DICT:\n",
        "                            val_score = booster_score_update(token_without_stop[check_index], val_score)\n",
        "                            aro_score = booster_score_update(token_without_stop[check_index], aro_score)\n",
        "                            check_index -= 1\n",
        "                            continue\n",
        "                        elif nltk.pos_tag([token_without_stop[check_index]])[0][1] in {'RB','RBR','RBS'}:\n",
        "                            check_index -= 1\n",
        "                            continue\n",
        "                        else:\n",
        "                            break\n",
        "\n",
        "                    tempV.append(val_score)\n",
        "                    tempA.append(aro_score)\n",
        "                \n",
        "                    \n",
        "                emo_index += 1\n",
        "            tok_index += 1 \n",
        "\n",
        "            if len(emoWord_lst) != 0:\n",
        "                score_lstV.append(tempV[word_similarity(emoWord_lst,token)])\n",
        "                score_lstA.append(tempA[word_similarity(emoWord_lst,token)])   \n",
        "\n",
        "        check += 1\n",
        "        if check % 1000 == 0: # 중간 저장 과정\n",
        "            print(check)  \n",
        "            result_DF = pd.DataFrame((zip(numTokens, numNVTokens, avgLexNVal,numPVTokens, avgLexPVal,numNATokens,avgLexNAro,numPATokens,avgLexPAro)),\n",
        "                                      columns = ['numTokens', 'NVnum', 'NVscore','PVnum', 'PVscore','NAnum','NAscore','PAnum','PAscore'])           \n",
        "            result_DF.to_csv(save_path, mode='w', index = False)\n",
        "                \n",
        "        if len(score_lstV) != 0: \n",
        "            neg_score = 0\n",
        "            pos_score = 0\n",
        "            countN = 0\n",
        "            countP = 0 \n",
        "            for i in range(len(score_lstV)):\n",
        "                if score_lstV[i] < 0:\n",
        "                    neg_score += score_lstV[i]\n",
        "                    countN +=1\n",
        "                elif score_lstV[i] > 0:\n",
        "                    pos_score += score_lstV[i]\n",
        "                    countP += 1\n",
        "            avgLexNVal.append(neg_score)\n",
        "            avgLexPVal.append(pos_score)\n",
        "            numNVTokens.append(countN)\n",
        "            numPVTokens.append(countP)\n",
        "\n",
        "            neg_score = 0\n",
        "            pos_score = 0\n",
        "            countN = 0\n",
        "            countP = 0 \n",
        "            for i in range(len(score_lstA)):\n",
        "                if score_lstA[i] < 0:\n",
        "                    neg_score += score_lstA[i]\n",
        "                    countN +=1\n",
        "                elif score_lstV[i] > 0:\n",
        "                    pos_score += score_lstA[i]\n",
        "                    countP +=1\n",
        "            avgLexNAro.append(neg_score)\n",
        "            avgLexPAro.append(pos_score)\n",
        "            numNATokens.append(countN)\n",
        "            numPATokens.append(countP)\n",
        "\n",
        "        else:\n",
        "            avgLexNVal.append(0)\n",
        "            avgLexPVal.append(0)\n",
        "            avgLexNAro.append(0)\n",
        "            avgLexPAro.append(0)\n",
        "            numNATokens.append(0)\n",
        "            numPATokens.append(0)\n",
        "            numNVTokens.append(0)\n",
        "            numPVTokens.append(0)\n",
        "        #생성되는 score list의 element 개수 df와 맞는지 체크할 것\n",
        "    \n",
        "    # 파일 저장 확인\n",
        "    result_DF = pd.DataFrame((zip(numTokens, numNVTokens, avgLexNVal,numPVTokens, avgLexPVal,numNATokens,avgLexNAro,numPATokens,avgLexPAro)),\n",
        "                              columns = ['numTokens', 'NVnum', 'NVscore','PVnum', 'PVscore','NAnum','NAscore','PAnum','PAscore'])\n",
        "    result_DF.to_csv(save_path, mode='w', index = False)\n",
        "    \n",
        "    return result_DF #numTokens, numTokens_without_stop, numLexTokens, avgLexVal,avgLexAro   \n",
        "\n",
        "# dep_score = score_by_lex_upgrade_together(lexicon_Val, lexicon_Aro, dep_cleansing,'./scoring_result/upgraded_twice/depression_score.csv')\n"
      ],
      "metadata": {
        "id": "8B2W0ZuwTcdQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c683c3-88db-469d-d309-7771de397fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dep_cleansing = pd.read_csv(\"./cleansing_dep.csv\")\n",
        "bipolar_cleansing = pd.read_csv(\"./cleansing_bip.csv\")\n",
        "anxiety_cleansing = pd.read_csv(\"./cleansing_pan.csv\")\n",
        "\n",
        "dep_cleansing = dep_cleansing['0']\n",
        "bipolar_cleansing = bipolar_cleansing['0']\n",
        "anxiety_cleansing = anxiety_cleansing['0']\n",
        "\n",
        "covid_cleansing = pd.read_csv(\"./cleansing_cov.csv\")\n",
        "# relationship_cleansing = pd.read_csv(\"./cleansing_rel.csv\")\n",
        "teaching_cleansing = pd.read_csv(\"./cleansing_tea.csv\")\n",
        "\n",
        "covid_cleansing = covid_cleansing['0']\n",
        "# relationship_cleansing = relationship_cleansing['0']\n",
        "teaching_cleansing = teaching_cleansing['0']"
      ],
      "metadata": {
        "id": "LdwkpyCyZyo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "panic_score =  score_by_lex_upgrade_NPInd(lexicon_Val, lexicon_Aro, anxiety_cleansing,'./scoring_result/negpos_independet/panic_score.csv')\n",
        "\n",
        "covid_score = score_by_lex_upgrade_NPInd(lexicon_Val, lexicon_Aro, covid_cleansing,'./scoring_result/negpos_independet/covid_score.csv')"
      ],
      "metadata": {
        "id": "k9qpnJHbZsVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depression_score = score_by_lex_upgrade_NPInd(lexicon_Val, lexicon_Aro, dep_cleansing,'./scoring_result/negpos_independet/depression_score.csv')\n",
        "\n",
        "bipolar_score = score_by_lex_upgrade_NPInd(lexicon_Val, lexicon_Aro, bipolar_cleansing,'./scoring_result/negpos_independet/bipolar_score.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ED0rWqAwauBZ",
        "outputId": "9dee2b73-2cff-46cf-c109-981a7f4c6be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "teaching_score = score_by_lex_upgrade_NPInd(lexicon_Val, lexicon_Aro, teaching_cleansing,'./scoring_result/negpos_independet/teaching_score.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06ZKwlTSavql",
        "outputId": "7a2544d0-00d1-494b-820c-fca1af048b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FU0uJYwoaxk0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}